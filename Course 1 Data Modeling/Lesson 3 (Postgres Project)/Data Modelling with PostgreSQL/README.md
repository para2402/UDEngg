# Data Modeling with Postgres

## **Project Description**
In this project, we perform data modeling with Postgres and build an ETL pipeline using Python.
 
## **Song Dataset**
Is a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/). Each file is in JSON format and contains metadata about a song and the artist of that song. Here is a sample record from the ```data/song_data/A/A/A/TRAAABD128F429CF47.json``` file in the song dataset:
```
{"num_songs": 1, "artist_id": "ARMJAGH1187FB546F3", "artist_latitude": 35.14968, "artist_longitude": -90.04892, "artist_location": "Memphis, TN", "artist_name": "The Box Tops", "song_id": "SOCIWDW12A8C13D406", "title": "Soul Deep", "duration": 148.03546, "year": 1969}
```

## **Log Dataset**
The log dataset consists of log files in JSON format generated by this event simulator based on the songs in the dataset above. These simulate activity logs from a music streaming app based on specified configurations. Logs dataset is generated by [Event Simulator](https://github.com/Interana/eventsim).

Here is a sample record from the log dataset:
```
{"artist":"N.E.R.D. FEATURING MALICE","auth":"Logged In","firstName":"Jayden","gender":"M","itemInSession":0,"lastName":"Fox","length":288.9922,"level":"free","location":"New Orleans-Metairie, LA","method":"PUT","page":"NextSong","registration":1541033612796.0,"sessionId":184,"song":"Am I High (Feat. Malice)","status":200,"ts":1541121934796,"userAgent":"\"Mozilla\/5.0 (Windows NT 6.3; WOW64) AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"","userId":"101"}
```

## Star Schema

#### Fact Table 
**songplays** - records in log data associated with song plays i.e. records with page `NextSong`

```
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
```

#### Dimension Tables
**users**  - users in the app
```
user_id, first_name, last_name, gender, level
```
**songs**  - songs in music database
```
song_id, title, artist_id, year, duration
```
**artists**  - artists in music database
```
artist_id, name, location, latitude, longitude
```
**time**  - timestamps of records in  **songplays**  broken down into specific units
```
start_time, hour, day, week, month, year, weekday
```

## ETL Pipeline
- Firstly, the song metadata files are processed to fill the **songs** and **artists** dimension tables.
- Then the log files are processed  to fill the **users** and **time** dimension tables and **songplays** fact table.
- To populate the **time** dimension the timestamps in the log file entries are parsed to generate hour, day, dayofweek, week, month, year fields.
- While populating the **users** table, if a conflict occurs, the level of the user is updated.


## Project Files
- ```test.ipynb``` displays the first few rows of each table to let you check your database.
- ```create_tables.py``` drops and creates your tables. You run this file to reset your tables before each time you run your ETL scripts.
- ```etl.ipynb``` reads and processes a single file from song_data and log_data and loads the data into your tables. This notebook contains detailed instructions on the ETL process for each of the tables.
- ```etl.py``` reads and processes files from song_data and log_data and loads them into your tables. You can fill this out based on your work in the ETL notebook.
- ```sql_queries.py``` contains all your sql queries, and is imported into the last three files above.

## Environment 
- Python 3.6

- PostgresSQL 9.5

- psycopg2 - Python Driver for PostgreSQL database


## Instructions
- Give executable permissions to ```create_tables.py``` and ```etl.py``` and run as shown:
> /home/workspace# ./create_tables.py 
> /home/workspace# ./etl.py 

- Alternatively, run ```create_tables.py``` and ```etl.py``` as shown:
> /home/workspace# python create_tables.py  
> /home/workspace# python etl.py

 ### References:
- Udacity project description is used to compile the ```README.md``` file