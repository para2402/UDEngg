{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Olist Database EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from configparser import ConfigParser\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# config = ConfigParser()\n",
    "# config.read('config.cfg')\n",
    "\n",
    "# S3 buckets\n",
    "# RAW_DATA = config['S3']['RAW_DATA']\n",
    "# STAGING_DATA = config['S3']['STAGING_DATA']\n",
    "\n",
    "# Redshift\n",
    "# REDSHIFT_ROLE = config['REDSHIFT']['REDSHIFT_S3_IAM_ROLE_ARN']\n",
    "\n",
    "# EMR\n",
    "# EC2_KEY_NAME = config['EMR']['EC2_KEY_NAME']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.jars.packages\", \"org.apache.hadoop:hadoop-aws:2.7.0\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://d157a0672586:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f8c3138ca90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def explain_df(df):\n",
    "    df.printSchema()\n",
    "    print()\n",
    "    df.show(2)\n",
    "    print()\n",
    "    df.describe().show()\n",
    "\n",
    "\n",
    "def unique_count(df):\n",
    "    print('Number of unique values in each column:')\n",
    "    print('---------------------------------------')\n",
    "    for c in df.columns:\n",
    "        print(c, '-', df.select(c).distinct().count())\n",
    "    print(f'\\nRow Count: {df.count()}')\n",
    "\n",
    "\n",
    "def null_count(df):\n",
    "    df.agg(*[F.count(F.when(F.isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## EDA: Customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "customers_df = spark.read.csv('data/olist_customers_dataset.csv',\n",
    "                              inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- customer_unique_id: string (nullable = true)\n",
      " |-- customer_zip_code_prefix: integer (nullable = true)\n",
      " |-- customer_city: string (nullable = true)\n",
      " |-- customer_state: string (nullable = true)\n",
      "\n",
      "\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|         customer_id|  customer_unique_id|customer_zip_code_prefix|       customer_city|customer_state|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "|06b8999e2fba1a1fb...|861eff4711a542e4b...|                   14409|              franca|            SP|\n",
      "|18955e83d337fd6b2...|290c77bc529b7ac93...|                    9790|sao bernardo do c...|            SP|\n",
      "+--------------------+--------------------+------------------------+--------------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "+-------+--------------------+--------------------+------------------------+-------------------+--------------+\n",
      "|summary|         customer_id|  customer_unique_id|customer_zip_code_prefix|      customer_city|customer_state|\n",
      "+-------+--------------------+--------------------+------------------------+-------------------+--------------+\n",
      "|  count|               99441|               99441|                   99441|              99441|         99441|\n",
      "|   mean|                null|                null|       35137.47458291851|               null|          null|\n",
      "| stddev|                null|                null|       29797.93899620612|               null|          null|\n",
      "|    min|00012a2ce6f8dcda2...|0000366f3b9a7992b...|                    1003|abadia dos dourados|            AC|\n",
      "|    max|ffffe8b65bbe3087b...|ffffd2657e2aad290...|                   99990|             zortea|            TO|\n",
      "+-------+--------------------+--------------------+------------------------+-------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explain_df(df=customers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+------------------------+-------------+--------------+\n",
      "|customer_id|customer_unique_id|customer_zip_code_prefix|customer_city|customer_state|\n",
      "+-----------+------------------+------------------------+-------------+--------------+\n",
      "|          0|                 0|                       0|            0|             0|\n",
      "+-----------+------------------+------------------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_count(df=customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** No nulls in customer table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in each column:\n",
      "---------------------------------------\n",
      "customer_id - 99441\n",
      "customer_unique_id - 96096\n",
      "customer_zip_code_prefix - 14994\n",
      "customer_city - 4119\n",
      "customer_state - 27\n",
      "\n",
      "Row Count: 99441\n"
     ]
    }
   ],
   "source": [
    "unique_count(df=customers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## EDA: Sellers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sellers_df = spark.read.csv('data/olist_sellers_dataset.csv',\n",
    "                            inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- seller_zip_code_prefix: integer (nullable = true)\n",
      " |-- seller_city: string (nullable = true)\n",
      " |-- seller_state: string (nullable = true)\n",
      "\n",
      "\n",
      "+--------------------+----------------------+-----------+------------+\n",
      "|           seller_id|seller_zip_code_prefix|seller_city|seller_state|\n",
      "+--------------------+----------------------+-----------+------------+\n",
      "|3442f8959a84dea7e...|                 13023|   campinas|          SP|\n",
      "|d1b65fc7debc3361e...|                 13844| mogi guacu|          SP|\n",
      "+--------------------+----------------------+-----------+------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "+-------+--------------------+----------------------+-----------+------------+\n",
      "|summary|           seller_id|seller_zip_code_prefix|seller_city|seller_state|\n",
      "+-------+--------------------+----------------------+-----------+------------+\n",
      "|  count|                3095|                  3095|       3095|        3095|\n",
      "|   mean|                null|    32291.059450726978|  4482255.0|        null|\n",
      "| stddev|                null|     32713.45382950901|        NaN|        null|\n",
      "|    min|0015a82c2db000af6...|                  1001|   04482255|          AC|\n",
      "|    max|ffff564a4f9085cd2...|                 99730|      xaxim|          SP|\n",
      "+-------+--------------------+----------------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explain_df(sellers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------------------+-----------+------------+\n",
      "|seller_id|seller_zip_code_prefix|seller_city|seller_state|\n",
      "+---------+----------------------+-----------+------------+\n",
      "|        0|                     0|          0|           0|\n",
      "+---------+----------------------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_count(sellers_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in each column:\n",
      "---------------------------------------\n",
      "seller_id - 3095\n",
      "seller_zip_code_prefix - 2246\n",
      "seller_city - 611\n",
      "seller_state - 23\n",
      "\n",
      "Row Count: 3095\n"
     ]
    }
   ],
   "source": [
    "unique_count(sellers_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## EDA: Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "products_df = spark.read.csv('data/olist_products_dataset.csv',\n",
    "                             inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- product_category_name: string (nullable = true)\n",
      " |-- product_name_lenght: integer (nullable = true)\n",
      " |-- product_description_lenght: integer (nullable = true)\n",
      " |-- product_photos_qty: integer (nullable = true)\n",
      " |-- product_weight_g: integer (nullable = true)\n",
      " |-- product_length_cm: integer (nullable = true)\n",
      " |-- product_height_cm: integer (nullable = true)\n",
      " |-- product_width_cm: integer (nullable = true)\n",
      "\n",
      "\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|          product_id|product_category_name|product_name_lenght|product_description_lenght|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|1e9e8ef04dbcff454...|           perfumaria|                 40|                       287|                 1|             225|               16|               10|              14|\n",
      "|3aa071139cb16b67c...|                artes|                 44|                       276|                 1|            1000|               30|               18|              20|\n",
      "+--------------------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "+-------+--------------------+---------------------+-------------------+--------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|summary|          product_id|product_category_name|product_name_lenght|product_description_lenght|product_photos_qty|  product_weight_g| product_length_cm| product_height_cm|  product_width_cm|\n",
      "+-------+--------------------+---------------------+-------------------+--------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "|  count|               32951|                32341|              32341|                     32341|             32341|             32949|             32949|             32949|             32949|\n",
      "|   mean|                null|                 null|  48.47694876472589|         771.4952846232337|2.1889861166939797|2276.4724877841513| 30.81507784758263|16.937661234028347|23.196728277034204|\n",
      "| stddev|                null|                 null| 10.245740725237287|         635.1152246349538|1.7367656379315435| 4282.038730977024|16.914458054065953|13.637554061749569|12.079047453227794|\n",
      "|    min|00066f42aeeb9f300...| agro_industria_e_...|                  5|                         4|                 1|                 0|                 7|                 2|                 6|\n",
      "|    max|fffe9eeff12fcbd74...| utilidades_domest...|                 76|                      3992|                20|             40425|               105|               105|               118|\n",
      "+-------+--------------------+---------------------+-------------------+--------------------------+------------------+------------------+------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explain_df(products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|product_id|product_category_name|product_name_lenght|product_description_lenght|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|\n",
      "+----------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|         0|                  610|                610|                       610|               610|               2|                2|                2|               2|\n",
      "+----------+---------------------+-------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_count(products_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "products_df.createOrReplaceTempView('product')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(product_id)|\n",
      "+-----------------+\n",
      "|              610|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT COUNT(product_id) FROM product\n",
    "             WHERE product_category_name IS NULL\n",
    "               AND product_name_lenght IS NULL\n",
    "               AND product_description_lenght IS NULL\n",
    "               AND product_photos_qty IS NULL\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|count(product_id)|\n",
      "+-----------------+\n",
      "|                1|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT COUNT(product_id) FROM product\n",
    "             WHERE product_category_name IS NULL\n",
    "               AND product_name_lenght IS NULL\n",
    "               AND product_description_lenght IS NULL\n",
    "               AND product_photos_qty IS NULL\n",
    "               AND product_weight_g IS NULL\n",
    "               AND product_length_cm IS NULL\n",
    "               AND product_height_cm IS NULL\n",
    "               AND product_width_cm IS NULL\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** There are around 610 products without any name and description. Also, there is one product with no details at all. Lets see if there are any orders with these products. If there are no orders with these products we can ignore these products while moving to the data warehouse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "Lets save the product_ids of these products for future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "products_partial_details = spark.sql(\"\"\"SELECT product_id FROM product\n",
    "                                        WHERE product_category_name IS NULL\n",
    "                                           AND product_name_lenght IS NULL\n",
    "                                           AND product_description_lenght IS NULL\n",
    "                                           AND product_photos_qty IS NULL\n",
    "                                      \"\"\").collect()\n",
    "\n",
    "products_partial_details = [r.product_id for r in products_partial_details]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "products_no_details = spark.sql(\"\"\"SELECT product_id FROM product\n",
    "                                   WHERE product_category_name IS NULL\n",
    "                                     AND product_name_lenght IS NULL\n",
    "                                     AND product_description_lenght IS NULL\n",
    "                                     AND product_photos_qty IS NULL\n",
    "                                     AND product_weight_g IS NULL\n",
    "                                     AND product_length_cm IS NULL\n",
    "                                     AND product_height_cm IS NULL\n",
    "                                     AND product_width_cm IS NULL\n",
    "                                \"\"\").collect()\n",
    "\n",
    "products_no_details = [r.product_id for r in products_no_details]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in each column:\n",
      "---------------------------------------\n",
      "product_id - 32951\n",
      "product_category_name - 74\n",
      "product_name_lenght - 67\n",
      "product_description_lenght - 2961\n",
      "product_photos_qty - 20\n",
      "product_weight_g - 2205\n",
      "product_length_cm - 100\n",
      "product_height_cm - 103\n",
      "product_width_cm - 96\n",
      "\n",
      "Row Count: 32951\n"
     ]
    }
   ],
   "source": [
    "unique_count(products_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## EDA: Orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "orders_df = spark.read.csv('data/olist_orders_dataset.csv',\n",
    "                           inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_at: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_date: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_date: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_date: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n",
      "|            order_id|         customer_id|order_status|order_purchase_timestamp|  order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|\n",
      "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n",
      "|e481f51cbdc54678b...|9ef432eb625129730...|   delivered|     2017-10-02 10:56:33|2017-10-02 11:07:15|         2017-10-04 19:55:00|          2017-10-10 21:25:13|          2017-10-18 00:00:00|\n",
      "|53cdb2fc8bc7dce0b...|b0830fb4747a6c6d2...|   delivered|     2018-07-24 20:41:37|2018-07-26 03:24:27|         2018-07-26 14:31:00|          2018-08-07 15:27:45|          2018-08-13 00:00:00|\n",
      "+--------------------+--------------------+------------+------------------------+-------------------+----------------------------+-----------------------------+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "+-------+--------------------+--------------------+------------+\n",
      "|summary|            order_id|         customer_id|order_status|\n",
      "+-------+--------------------+--------------------+------------+\n",
      "|  count|               99441|               99441|       99441|\n",
      "|   mean|                null|                null|        null|\n",
      "| stddev|                null|                null|        null|\n",
      "|    min|00010242fe8c5a6d1...|00012a2ce6f8dcda2...|    approved|\n",
      "|    max|fffe41c64501cc87c...|ffffe8b65bbe3087b...| unavailable|\n",
      "+-------+--------------------+--------------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explain_df(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+\n",
      "|order_id|customer_id|order_status|order_purchase_timestamp|order_approved_at|order_delivered_carrier_date|order_delivered_customer_date|order_estimated_delivery_date|\n",
      "+--------+-----------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+\n",
      "|       0|          0|           0|                       0|              160|                        1783|                         2965|                            0|\n",
      "+--------+-----------+------------+------------------------+-----------------+----------------------------+-----------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_count(df=orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in each column:\n",
      "---------------------------------------\n",
      "order_id - 99441\n",
      "customer_id - 99441\n",
      "order_status - 8\n",
      "order_purchase_timestamp - 98875\n",
      "order_approved_at - 90734\n",
      "order_delivered_carrier_date - 81019\n",
      "order_delivered_customer_date - 95665\n",
      "order_estimated_delivery_date - 459\n",
      "\n",
      "Row Count: 99441\n"
     ]
    }
   ],
   "source": [
    "unique_count(orders_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "orders_df.createOrReplaceTempView('order')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------+-----------------------------+\n",
      "|min(order_purchase_timestamp)|max(order_purchase_timestamp)|\n",
      "+-----------------------------+-----------------------------+\n",
      "|          2016-09-04 21:15:19|          2018-10-17 17:30:18|\n",
      "+-----------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT MIN(order_purchase_timestamp),\n",
    "                     MAX(order_purchase_timestamp)\n",
    "             FROM order\n",
    "           \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|order_status_types|\n",
      "+------------------+\n",
      "|           shipped|\n",
      "|          canceled|\n",
      "|          approved|\n",
      "|          invoiced|\n",
      "|           created|\n",
      "|         delivered|\n",
      "|       unavailable|\n",
      "|        processing|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT DISTINCT order_status AS order_status_types\n",
    "             FROM order\n",
    "           \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** Dataset has almost 2 years of sales data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# EDA: Order Items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "order_items_df = spark.read.csv('data/olist_order_items_dataset.csv',\n",
    "                                inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- order_item_id: integer (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- shipping_limit_date: timestamp (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      "\n",
      "\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "|            order_id|order_item_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "|00010242fe8c5a6d1...|            1|4244733e06e7ecb49...|48436dade18ac8b2b...|2017-09-19 09:45:35| 58.9|        13.29|\n",
      "|00018f77f2f0320c5...|            1|e5f2d52b802189ee6...|dd7ddc04e1b6c2c61...|2017-05-03 11:05:13|239.9|        19.93|\n",
      "+--------------------+-------------+--------------------+--------------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "+-------+--------------------+------------------+--------------------+--------------------+------------------+------------------+\n",
      "|summary|            order_id|     order_item_id|          product_id|           seller_id|             price|     freight_value|\n",
      "+-------+--------------------+------------------+--------------------+--------------------+------------------+------------------+\n",
      "|  count|              112650|            112650|              112650|              112650|            112650|            112650|\n",
      "|   mean|                null|1.1978339991122948|                null|                null|120.65373901477311| 19.99031992898562|\n",
      "| stddev|                null|0.7051240313951734|                null|                null| 183.6339280502597|15.806405412296998|\n",
      "|    min|00010242fe8c5a6d1...|                 1|00066f42aeeb9f300...|0015a82c2db000af6...|              0.85|               0.0|\n",
      "|    max|fffe41c64501cc87c...|                21|fffe9eeff12fcbd74...|ffff564a4f9085cd2...|            6735.0|            409.68|\n",
      "+-------+--------------------+------------------+--------------------+--------------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explain_df(order_items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-------------+----------+---------+-------------------+-----+-------------+\n",
      "|order_id|order_item_id|product_id|seller_id|shipping_limit_date|price|freight_value|\n",
      "+--------+-------------+----------+---------+-------------------+-----+-------------+\n",
      "|       0|            0|         0|        0|                  0|    0|            0|\n",
      "+--------+-------------+----------+---------+-------------------+-----+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_count(order_items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in each column:\n",
      "---------------------------------------\n",
      "order_id - 98666\n",
      "order_item_id - 21\n",
      "product_id - 32951\n",
      "seller_id - 3095\n",
      "shipping_limit_date - 93318\n",
      "price - 5968\n",
      "freight_value - 6999\n",
      "\n",
      "Row Count: 112650\n"
     ]
    }
   ],
   "source": [
    "unique_count(order_items_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "order_items_df.createOrReplaceTempView('order_item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------------------------------+\n",
      "|count(DISTINCT order_id, order_item_id, product_id, seller_id)|\n",
      "+--------------------------------------------------------------+\n",
      "|                                                        112650|\n",
      "+--------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT COUNT(DISTINCT order_id, order_item_id, product_id, seller_id)\n",
    "             FROM order_item\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** It looks like `order_id, order_item_id, product_id, seller_id` together form a PK for the order_items table in the operational data source. In fact, on grouping the rows based on `order_id, product_id, seller_id` and counting the number of rows gives the quantity/ #units of `product_id` are ordered from the seller with `seller_id` in the order with order_id `order_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|sum(qty)|\n",
      "+--------+\n",
      "|  112650|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT SUM(qty)\n",
    "             FROM (SELECT order_id, product_id, seller_id, COUNT(order_item_id) AS qty\n",
    "                   FROM order_item\n",
    "                   GROUP BY order_id, product_id, seller_id\n",
    "                   HAVING qty) AS tempTable\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check if there are any orders with products having missing details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+\n",
      "|product_id|numberOfOrders|\n",
      "+----------+--------------+\n",
      "+----------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT product_id, COUNT(order_id) numberOfOrders\n",
    "             FROM order_item\n",
    "             WHERE product_id IN (SELECT product_id FROM product\n",
    "                                  WHERE product_category_name IS NULL\n",
    "                                    AND product_name_lenght IS NULL\n",
    "                                    AND product_description_lenght IS NULL\n",
    "                                    AND product_photos_qty IS NULL)\n",
    "             GROUP BY product_id\n",
    "             HAVING COUNT(order_id) = 0\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** Products with missing name and description are present in atleast one order. So the products with incomplete details cannot be ignored while moving to the warehouse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------+\n",
      "|          product_id|numberOfOrders|\n",
      "+--------------------+--------------+\n",
      "|5eb564652db742ff8...|            17|\n",
      "+--------------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(f\"\"\"SELECT product_id, COUNT(order_id) numberOfOrders\n",
    "             FROM order_item\n",
    "             WHERE product_id = '{products_no_details[0]}'\n",
    "             GROUP BY product_id\n",
    "           \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** Interestingly there are 17 orders containing the product with no details!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### How to get number of units of a product per order?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+-----------------------------------+\n",
      "|order_id|product_id|seller_id|count(DISTINCT shipping_limit_date)|\n",
      "+--------+----------+---------+-----------------------------------+\n",
      "+--------+----------+---------+-----------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT order_id, product_id, seller_id, COUNT(DISTINCT shipping_limit_date)\n",
    "             FROM order_item\n",
    "             GROUP BY order_id, product_id, seller_id\n",
    "             HAVING COUNT(DISTINCT shipping_limit_date) > 1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+---------------------+\n",
      "|order_id|product_id|seller_id|count(DISTINCT price)|\n",
      "+--------+----------+---------+---------------------+\n",
      "+--------+----------+---------+---------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT order_id, product_id, seller_id, COUNT(DISTINCT price)\n",
    "             FROM order_item\n",
    "             GROUP BY order_id, product_id, seller_id\n",
    "             HAVING COUNT(DISTINCT price) > 1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+---------+-----------------------------+\n",
      "|order_id|product_id|seller_id|count(DISTINCT freight_value)|\n",
      "+--------+----------+---------+-----------------------------+\n",
      "+--------+----------+---------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT order_id, product_id, seller_id, COUNT(DISTINCT freight_value)\n",
    "             FROM order_item\n",
    "             GROUP BY order_id, product_id, seller_id\n",
    "             HAVING COUNT(DISTINCT freight_value) > 1\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** The `order_items` table contains 1 row for each unit of a product in an `order_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-------------------+-----+-------------+---+\n",
      "|            order_id|          product_id|           seller_id|shipping_limit_date|price|freight_value|qty|\n",
      "+--------------------+--------------------+--------------------+-------------------+-----+-------------+---+\n",
      "|00c9d9e61ed13f5bf...|2ffb8b836bb62d53a...|391fc6631aebcf300...|2017-03-02 13:25:10|44.55|        11.74|  1|\n",
      "+--------------------+--------------------+--------------------+-------------------+-----+-------------+---+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT order_id, product_id, seller_id,\n",
    "                    shipping_limit_date, price, freight_value,\n",
    "                    COUNT(order_item_id) AS qty\n",
    "             FROM order_item\n",
    "             GROUP BY order_id, product_id, seller_id,\n",
    "                      shipping_limit_date, price, freight_value\n",
    "          \"\"\").show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## EDA: Payments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "payments_df = spark.read.csv('data/olist_order_payments_dataset.csv',\n",
    "                             inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- payment_sequential: integer (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- payment_installments: integer (nullable = true)\n",
      " |-- payment_value: double (nullable = true)\n",
      "\n",
      "\n",
      "+--------------------+------------------+------------+--------------------+-------------+\n",
      "|            order_id|payment_sequential|payment_type|payment_installments|payment_value|\n",
      "+--------------------+------------------+------------+--------------------+-------------+\n",
      "|b81ef226f3fe1789b...|                 1| credit_card|                   8|        99.33|\n",
      "|a9810da82917af2d9...|                 1| credit_card|                   1|        24.39|\n",
      "+--------------------+------------------+------------+--------------------+-------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "+-------+--------------------+------------------+------------+--------------------+------------------+\n",
      "|summary|            order_id|payment_sequential|payment_type|payment_installments|     payment_value|\n",
      "+-------+--------------------+------------------+------------+--------------------+------------------+\n",
      "|  count|              103886|            103886|      103886|              103886|            103886|\n",
      "|   mean|                null|1.0926785129853878|        null|   2.853348863176944|154.10038041698365|\n",
      "| stddev|                null|0.7065837791949948|        null|   2.687050673856486|217.49406386472384|\n",
      "|    min|00010242fe8c5a6d1...|                 1|      boleto|                   0|               0.0|\n",
      "|    max|fffe41c64501cc87c...|                29|     voucher|                  24|          13664.08|\n",
      "+-------+--------------------+------------------+------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explain_df(payments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------+------------+--------------------+-------------+\n",
      "|order_id|payment_sequential|payment_type|payment_installments|payment_value|\n",
      "+--------+------------------+------------+--------------------+-------------+\n",
      "|       0|                 0|           0|                   0|            0|\n",
      "+--------+------------------+------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_count(payments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in each column:\n",
      "---------------------------------------\n",
      "order_id - 99440\n",
      "payment_sequential - 29\n",
      "payment_type - 5\n",
      "payment_installments - 24\n",
      "payment_value - 29077\n",
      "\n",
      "Row Count: 103886\n"
     ]
    }
   ],
   "source": [
    "unique_count(payments_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "payments_df.createOrReplaceTempView('payment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+------------------+------------+--------------------+-------------+\n",
      "|order_id                        |payment_sequential|payment_type|payment_installments|payment_value|\n",
      "+--------------------------------+------------------+------------+--------------------+-------------+\n",
      "|7a5472f7c8cecc2e1cf43d12271e4eca|1                 |credit_card |8                   |228.75       |\n",
      "|7a5472f7c8cecc2e1cf43d12271e4eca|2                 |voucher     |1                   |62.87        |\n",
      "+--------------------------------+------------------+------------+--------------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT *\n",
    "             FROM payment\n",
    "             WHERE order_id = '7a5472f7c8cecc2e1cf43d12271e4eca'\n",
    "          \"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** Since the scope of the data warehouse/ mart is sales analysis, we can just take the `payment_value` and aggregate over the `order_id` and store it in the fact table as a measure indicating the total order value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## EDA: Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "reviews_df = spark.read.csv('data/olist_order_reviews_dataset.csv',\n",
    "                            inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- review_score: string (nullable = true)\n",
      " |-- review_comment_title: string (nullable = true)\n",
      " |-- review_comment_message: string (nullable = true)\n",
      " |-- review_creation_date: string (nullable = true)\n",
      " |-- review_answer_timestamp: string (nullable = true)\n",
      "\n",
      "\n",
      "+--------------------+--------------------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|           review_id|            order_id|review_score|review_comment_title|review_comment_message|review_creation_date|review_answer_timestamp|\n",
      "+--------------------+--------------------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|7bc2406110b926393...|73fc7af87114b3971...|           4|                null|                  null| 2018-01-18 00:00:00|    2018-01-18 21:46:59|\n",
      "|80e641a11e56f04c1...|a548910a1c6147796...|           5|                null|                  null| 2018-03-10 00:00:00|    2018-03-11 03:05:13|\n",
      "+--------------------+--------------------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "+-------+------------------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|summary|         review_id|            order_id|        review_score|review_comment_title|review_comment_message|review_creation_date|review_answer_timestamp|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|  count|            105188|              102859|              102692|               12176|                 41868|               96025|                  96002|\n",
      "|   mean|               4.5|                 0.0|   4.071667849964501|3.155449396525236...|  1.111111111111111...|                null|                   null|\n",
      "| stddev|0.7071067811865476|                 0.0|   1.386648877434689|5.616554832455847E11|                   NaN|                null|                   null|\n",
      "|    min|                 \"|                    |                   \"|                    |                      | FOI A MINHA PRIM...|    POIS NO CUMPREM...|\n",
      "|    max|     \"|visando sempre o ...|seria mais coeren...|                  |  |veio bem embalada...|              70 + R$15|\n",
      "+-------+------------------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explain_df(reviews_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|review_id|order_id|review_score|review_comment_title|review_comment_message|review_creation_date|review_answer_timestamp|\n",
      "+---------+--------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|        1|    2330|        2497|               93013|                 63321|                9164|                   9187|\n",
      "+---------+--------+------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_count(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** We could drop the reviews with NULL `order_id` and NULL `review_id` while populating the datawarehouse dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in each column:\n",
      "---------------------------------------\n",
      "review_id - 103957\n",
      "order_id - 100563\n",
      "review_score - 2568\n",
      "review_comment_title - 5045\n",
      "review_comment_message - 36664\n",
      "review_creation_date - 726\n",
      "review_answer_timestamp - 95073\n",
      "\n",
      "Row Count: 105189\n"
     ]
    }
   ],
   "source": [
    "unique_count(reviews_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Are there any orders with more than 1 review?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "reviews_df.createOrReplaceTempView('review')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|count(order_id)|\n",
      "+---------------+\n",
      "|            967|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT COUNT(order_id)\n",
    "             FROM (SELECT order_id\n",
    "                   FROM review\n",
    "                   GROUP BY order_id\n",
    "                   HAVING COUNT(review_id) > 1) AS tempTable\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** There are 967 orders with more than 1 review. Let's record only average `review_score` in the fact table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## EDA: Geolocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "geo_df = spark.read.csv('data/olist_geolocation_dataset.csv',\n",
    "                        inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- geolocation_zip_code_prefix: integer (nullable = true)\n",
      " |-- geolocation_lat: double (nullable = true)\n",
      " |-- geolocation_lng: double (nullable = true)\n",
      " |-- geolocation_city: string (nullable = true)\n",
      " |-- geolocation_state: string (nullable = true)\n",
      "\n",
      "\n",
      "+---------------------------+-------------------+------------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_lat|   geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+-------------------+------------------+----------------+-----------------+\n",
      "|                       1037| -23.54562128115268|-46.63929204800168|       sao paulo|               SP|\n",
      "|                       1046|-23.546081127035535|-46.64482029837157|       sao paulo|               SP|\n",
      "+---------------------------+-------------------+------------------+----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "+-------+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|summary|geolocation_zip_code_prefix|    geolocation_lat|    geolocation_lng|geolocation_city|geolocation_state|\n",
      "+-------+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "|  count|                    1000163|            1000163|            1000163|         1000163|          1000163|\n",
      "|   mean|          36574.16646586607|-21.176152910383102|-46.390541320935995|            null|             null|\n",
      "| stddev|          30549.33571031949|  5.715866308823084|  4.269748306619793|            null|             null|\n",
      "|    min|                       1001|  -36.6053744107061|-101.46676644931476|        * cidade|               AC|\n",
      "|    max|                      99990|  45.06593318269697| 121.10539381057764|            leo|               TO|\n",
      "+-------+---------------------------+-------------------+-------------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "explain_df(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+---------------+---------------+----------------+-----------------+\n",
      "|geolocation_zip_code_prefix|geolocation_lat|geolocation_lng|geolocation_city|geolocation_state|\n",
      "+---------------------------+---------------+---------------+----------------+-----------------+\n",
      "|                          0|              0|              0|               0|                0|\n",
      "+---------------------------+---------------+---------------+----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "null_count(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique values in each column:\n",
      "---------------------------------------\n",
      "geolocation_zip_code_prefix - 19015\n",
      "geolocation_lat - 717372\n",
      "geolocation_lng - 717615\n",
      "geolocation_city - 8011\n",
      "geolocation_state - 27\n",
      "\n",
      "Row Count: 1000163\n"
     ]
    }
   ],
   "source": [
    "unique_count(geo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "geo_df.createOrReplaceTempView('geolocation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+-------------------+\n",
      "|geolocation_zip_code_prefix|distinctCoordinates|\n",
      "+---------------------------+-------------------+\n",
      "|                       1238|                164|\n",
      "|                       2122|                 33|\n",
      "|                       2142|                  5|\n",
      "|                       2366|                 33|\n",
      "|                       2866|                 41|\n",
      "|                       3175|                 32|\n",
      "|                       3918|                 50|\n",
      "|                       4101|                 72|\n",
      "|                       4935|                 15|\n",
      "|                       5518|                 27|\n",
      "|                       6397|                 95|\n",
      "|                       6654|                187|\n",
      "|                       6620|                 66|\n",
      "|                       7240|                155|\n",
      "|                       8592|                 18|\n",
      "|                       9852|                107|\n",
      "|                      12940|                253|\n",
      "|                      13289|                 61|\n",
      "|                      13285|                 49|\n",
      "|                      13840|                118|\n",
      "+---------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT geolocation_zip_code_prefix, COUNT(*) AS distinctCoordinates\n",
    "             FROM geolocation\n",
    "             GROUP BY geolocation_zip_code_prefix\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+--------------------+-----------------+-------------------+\n",
      "|geolocation_zip_code_prefix|    geolocation_city|geolocation_state|distinctCoordinates|\n",
      "+---------------------------+--------------------+-----------------+-------------------+\n",
      "|                       4044|           sao paulo|               SP|                 39|\n",
      "|                       5503|           so paulo|               SP|                  4|\n",
      "|                       8225|           sao paulo|               SP|                 83|\n",
      "|                       9230|         santo andre|               SP|                163|\n",
      "|                      11050|              santos|               SP|                138|\n",
      "|                      17320|   mineiros do tiete|               SP|                 42|\n",
      "|                      21735|      rio de janeiro|               RJ|                 65|\n",
      "|                      23098|      rio de janeiro|               RJ|                 29|\n",
      "|                      29645|santa maria de je...|               ES|                 20|\n",
      "|                      35680|              itana|               MG|                 71|\n",
      "|                      47150|santa rita de cassia|               BA|                 21|\n",
      "|                      49480|          simao dias|               SE|                 27|\n",
      "|                      85805|            cascavel|               PR|                 75|\n",
      "|                      90470|        porto alegre|               RS|                114|\n",
      "|                      96790|    barra do ribeiro|               RS|                 47|\n",
      "|                      97030|         santa maria|               RS|                 81|\n",
      "|                       4802|           sao paulo|               SP|                 28|\n",
      "|                       5093|           sao paulo|               SP|                 33|\n",
      "|                       6710|               cotia|               SP|                 62|\n",
      "|                      14750|        pitangueiras|               SP|                140|\n",
      "+---------------------------+--------------------+-----------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"SELECT geolocation_zip_code_prefix, geolocation_city, geolocation_state,\n",
    "             COUNT(DISTINCT geolocation_lat, geolocation_lng) AS distinctCoordinates\n",
    "             FROM geolocation\n",
    "             GROUP BY geolocation_zip_code_prefix, geolocation_city, geolocation_state\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "***Observation:*** Since each (zip_code, city, state) combination has more than 1 distinct coordinate, it is NOT clear what those coordinates indicate. Do they indicate coordinates from which a customer has placed an order? If so which order is placed from which coordinate? There is no connection to neither the orders nor the orders_items table. **For the purpose of building sales data warehouse we can ignore the geolocation table completely**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Sales Data Warehouse Schema\n",
    "***Data Warehouse/ Mart Scope:*** For Sales analysis targeted towards the sales deparment.  \n",
    "\n",
    "***Granularity/ Level of Detail of the Data Warehouse:*** Daily  \n",
    "\n",
    "***Kinds of questions to be answered by the warehouse:***  \n",
    "- No. of orders received in each month/ year\n",
    "- Avg total of orders received in each day/ month/ year\n",
    "- Weekly/ Monthly/ yearly Avg spendings of each customer\n",
    "- Weekly/ Monthly/ yearly Avg sales of each product type\n",
    "- Weekly/ Monthly/ yearly Avg sales made by each seller\n",
    "- Weekly/ Monthly/ yearly city/ state wise sales \n",
    "\n",
    "***Possible Project Extension:*** Additionally we could also have a mart for Accounting (fact-payments) and Marketing (fact-reviews) departments  \n",
    "\n",
    "***Type of dimensional model:*** STAR SCHEMA\n",
    "\n",
    "##DIMENSIONS:\n",
    "\n",
    "```plsql\n",
    "customer(customer_id, customer_unique_id, customer_zip_code_prefix, customer_city, customer_state)  \n",
    "  seller(seller_id, seller_zip_code_prefix, seller_city, seller_state)\n",
    " product(product_id,\n",
    "         product_category,            --> Translated using \"product_category_name_translation.csv\"\n",
    "         product_photos_qty,\n",
    "         product_weight_g,\n",
    "         product_length_cm,\n",
    "         product_height_cm,\n",
    "         product_width_cm)\n",
    "calender(calender_id, date, day,\n",
    "         dayofweek, month, year)\n",
    "```\n",
    "\n",
    "##FACT Table:\n",
    "\n",
    "```plsql\n",
    "order_item(\n",
    "           order_item_id,                          --> Artificial PK\n",
    "           order_id,                               --> orders\n",
    "           customer_id,                            --> orders\n",
    "           product_id,                             --> order_items\n",
    "           seller_id,                              --> order_items\n",
    "           purchase_date,                          --> orders (from \"order_purchase_timestamp\")\n",
    "           purchase_year,                          --> orders (from \"order_purchase_timestamp\")\n",
    "           purchase_month,                         --> orders (from \"order_purchase_timestamp\")\n",
    "           order_purchase_timestamp,               --> orders (from \"order_purchase_timestamp\")\n",
    "           order_approved_timestamp,               --> orders (from \"order_approved_at\")\n",
    "           order_delivered_carrier_timestamp,      --> orders (EXTRACT DATE from \"order_delivered_carrier_date\")\n",
    "           order_delivered_customer_timestamp,     --> orders (EXTRACT DATE from \"order_delivered_customer_date\")\n",
    "           order_estimated_delivery_timestamp,     --> orders (EXTRACT DATE from \"order_estimated_delivery_date\")\n",
    "           order_status,                           --> orders\n",
    "           shipping_limit_date,                    --> order_items\n",
    "           freight_value,                          --> order_items\n",
    "           unit_price,                             --> order_items (rename \"price\" -> \"unit_price\")\n",
    "           qty,                                    --> order_items (derived measure)\n",
    "           total_product_price,                    --> (qty * product_unit_price)\n",
    "           total_order_price,                      --> order_payments (derived measure)\n",
    "           avg_review_score,                       --> order_reviews (derived measure)\n",
    "          )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# ETL Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Customer Dimension  \n",
    "\n",
    "No transformations needed for customers table. Just saving in parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "customers_df.write.parquet('staging_data/customer', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Sellers Dimension  \n",
    "No transformations needed for sellers table. Just saving in parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "sellers_df.write.parquet('staging_data/seller', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Product Dimension  \n",
    "1. Join the `products` table with the `product_category_names` table in `product_category_name_translation.csv` to get the product names in english.  \n",
    "2. We can choose to drop the column `product_name_lenght` since we have translated the category name to english and the length will no more be relevant for sales analysis.\n",
    "3. Then save the `product` dimension in parquet format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "category_translation = spark.read.csv('data/product_category_name_translation.csv',\n",
    "                                      inferSchema=True, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------------------+\n",
      "|product_category_name|product_category_name_english|\n",
      "+---------------------+-----------------------------+\n",
      "|         beleza_saude|                health_beauty|\n",
      "| informatica_acess...|         computers_accesso...|\n",
      "+---------------------+-----------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "category_translation.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "category_translation.createOrReplaceTempView('translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "product_dim = spark.sql(\"\"\"SELECT p.*, t.product_category_name_english AS product_category\n",
    "                           FROM product p\n",
    "                           LEFT JOIN translation t\n",
    "                                   ON p.product_category_name = t.product_category_name\n",
    "                        \"\"\") \\\n",
    "                   .drop('product_category_name', 'product_name_lenght') \\\n",
    "                   .withColumnRenamed('product_description_lenght',\n",
    "                                      'product_description_length') ## correct the column name \"lenght\" to \"length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+----------------+\n",
      "|          product_id|product_description_length|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|product_category|\n",
      "+--------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+----------------+\n",
      "|1e9e8ef04dbcff454...|                       287|                 1|             225|               16|               10|              14|       perfumery|\n",
      "+--------------------+--------------------------+------------------+----------------+-----------------+-----------------+----------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "product_dim.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "product_dim.write.parquet('staging_data/product', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Calender Dimension\n",
    "\n",
    "Using the `order_purchase_timestamp` from the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "calender_dim = spark.sql(\"\"\"SELECT DISTINCT CAST(order_purchase_timestamp AS DATE) as date\n",
    "                            FROM order\n",
    "\n",
    "                            UNION\n",
    "\n",
    "                            SELECT DISTINCT CAST(order_approved_at AS DATE) as date\n",
    "                            FROM order\n",
    "\n",
    "                            UNION\n",
    "\n",
    "                            SELECT DISTINCT CAST(order_delivered_carrier_date AS DATE) as date\n",
    "                            FROM order\n",
    "\n",
    "                            UNION\n",
    "\n",
    "                            SELECT DISTINCT CAST(order_delivered_customer_date AS DATE) as date\n",
    "                            FROM order\n",
    "\n",
    "                            UNION\n",
    "\n",
    "                            SELECT DISTINCT CAST(order_estimated_delivery_date AS DATE) as date\n",
    "                            FROM order\n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "calender_dim.createOrReplaceTempView('calender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "calender_dim = spark.sql(\"\"\"SELECT date,\n",
    "                                   year(date) AS year,\n",
    "                                   month(date) AS month,\n",
    "                                   day(date) AS day,\n",
    "                                   dayofweek(date) AS dayOfWeek\n",
    "                            FROM calender\n",
    "                            WHERE date IS NOT NULL\n",
    "                         \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- date: date (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- month: integer (nullable = true)\n",
      " |-- day: integer (nullable = true)\n",
      " |-- dayOfWeek: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calender_dim.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-----+---+---------+\n",
      "|      date|year|month|day|dayOfWeek|\n",
      "+----------+----+-----+---+---------+\n",
      "|2017-09-11|2017|    9| 11|        2|\n",
      "|2018-08-10|2018|    8| 10|        6|\n",
      "+----------+----+-----+---+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "calender_dim.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "calender_dim.write.parquet('staging_data/calender', mode='overwrite')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Fact Table: `order_item`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Getting required fields from `orders` and `order_items`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "order_item_fact = spark.sql(\"\"\"SELECT monotonically_increasing_id() AS order_item_id,\n",
    "                                      o.order_id,                                           ----> Retaining Business Key for analysis\n",
    "                                      o.customer_id, i.product_id, i.seller_id,\n",
    "                                      CAST(o.order_purchase_timestamp AS DATE)              AS purchase_date,\n",
    "                                      CAST(year(o.order_purchase_timestamp) AS SMALLINT)    AS purchase_year,\n",
    "                                      CAST(month(o.order_purchase_timestamp) AS SMALLINT)   AS purchase_month,\n",
    "                                      o.order_purchase_timestamp,\n",
    "                                      o.order_approved_at                                   AS order_approved_timestamp,\n",
    "                                      o.order_delivered_carrier_date                        AS order_delivered_carrier_timestamp,\n",
    "                                      o.order_delivered_customer_date                       AS order_delivered_customer_timestamp,\n",
    "                                      o.order_estimated_delivery_date                       AS order_estimated_delivery_timestamp,\n",
    "                                      o.order_status,\n",
    "                                      i.shipping_limit_date,\n",
    "                                      \n",
    "                                      ----------------------------- MEASURES -----------------------------\n",
    "                                      \n",
    "                                      CAST(i.freight_value AS DECIMAL(5, 2))                AS freight_value,\n",
    "                                      CAST(i.price AS DECIMAL(10, 2))                       AS unit_price,\n",
    "                                      CAST(i.qty AS SMALLINT)                               AS qty,\n",
    "                                      CAST(ROUND(i.qty * i.price, 2) AS DECIMAL(10, 2))     AS total_product_price,\n",
    "                                      \n",
    "                                      (SELECT CAST(SUM(p.payment_value) AS DECIMAL(10, 2))\n",
    "                                       FROM payment AS p\n",
    "                                       WHERE p.order_id = o.order_id)                       AS total_order_price,\n",
    "                                       \n",
    "                                      (SELECT CAST(ROUND(AVG(r.review_score), 1) AS DECIMAL(2, 1))\n",
    "                                       FROM review AS r\n",
    "                                       WHERE r.order_id = o.order_id)                       AS avg_review_score\n",
    "                               FROM order AS o\n",
    "                               INNER JOIN (SELECT order_id, product_id, seller_id,\n",
    "                                                  shipping_limit_date, price, freight_value,\n",
    "                                                  COUNT(order_item_id) AS qty\n",
    "                                           FROM order_item\n",
    "                                           GROUP BY order_id, product_id, seller_id,\n",
    "                                                    shipping_limit_date, price, freight_value) AS i\n",
    "                                       ON o.order_id = i.order_id\n",
    "                          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- order_item_id: long (nullable = false)\n",
      " |-- order_id: string (nullable = true)\n",
      " |-- customer_id: string (nullable = true)\n",
      " |-- product_id: string (nullable = true)\n",
      " |-- seller_id: string (nullable = true)\n",
      " |-- purchase_date: date (nullable = true)\n",
      " |-- purchase_year: integer (nullable = true)\n",
      " |-- purchase_month: integer (nullable = true)\n",
      " |-- order_purchase_timestamp: timestamp (nullable = true)\n",
      " |-- order_approved_timestamp: timestamp (nullable = true)\n",
      " |-- order_delivered_carrier_timestamp: timestamp (nullable = true)\n",
      " |-- order_delivered_customer_timestamp: timestamp (nullable = true)\n",
      " |-- order_estimated_delivery_timestamp: timestamp (nullable = true)\n",
      " |-- order_status: string (nullable = true)\n",
      " |-- shipping_limit_date: timestamp (nullable = true)\n",
      " |-- freight_value: double (nullable = true)\n",
      " |-- unit_price: double (nullable = true)\n",
      " |-- qty: long (nullable = false)\n",
      " |-- total_product_price: double (nullable = true)\n",
      " |-- total_order_price: double (nullable = true)\n",
      " |-- avg_review_score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_item_fact.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------------+--------------------+--------------------+--------------------+-----------+-------------+--------------+------------------------+------------------------+---------------------------------+----------------------------------+----------------------------------+------------+-------------------+-------------+----------+---+-------------------+-----------------+----------------+\n",
      "|order_item_id|            order_id|         customer_id|          product_id|           seller_id|calender_id|purchase_year|purchase_month|order_purchase_timestamp|order_approved_timestamp|order_delivered_carrier_timestamp|order_delivered_customer_timestamp|order_estimated_delivery_timestamp|order_status|shipping_limit_date|freight_value|unit_price|qty|total_product_price|total_order_price|avg_review_score|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+-----------+-------------+--------------+------------------------+------------------------+---------------------------------+----------------------------------+----------------------------------+------------+-------------------+-------------+----------+---+-------------------+-----------------+----------------+\n",
      "|            0|014405982914c2cde...|2de342d6e5905a5a8...|6782d593f63105318...|325f3178fb58e2a97...| 2017-07-26|         2017|             7|     2017-07-26 17:38:47|     2017-07-26 17:50:17|              2017-07-27 19:39:52|               2017-07-31 15:53:33|               2017-08-17 00:00:00|   delivered|2017-08-01 17:50:17|         3.81|      27.9|  1|               27.9|            78.43|             5.0|\n",
      "+-------------+--------------------+--------------------+--------------------+--------------------+-----------+-------------+--------------+------------------------+------------------------+---------------------------------+----------------------------------+----------------------------------+------------+-------------------+-------------+----------+---+-------------------+-----------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order_item_fact.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# order_item_fact.write.parquet('staging_data/order_item',\n",
    "#                               mode='overwrite',\n",
    "#                               partitionBy=['purchase_year', 'purchase_month'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "102425"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.read.parquet('staging_data/order_item').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
